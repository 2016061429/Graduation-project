{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d0996a-3636-4211-ac77-021444a8bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting shap\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9e/3f/247e0017d52eeef37c170d71357eb3a12a2c06718d2e184c9929b6f3d9ed/shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.24.1)\n",
      "Requirement already satisfied: scipy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (from shap) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (23.0)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/13/b66627125b35f2987bd9872cf028b5e1e1ffcbc8d1e182ac4e84eed3998f/numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba->shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/57/7d/ef28d5812f852b93bd2a583d00cdcde56833d31b645ae0eaa7e71eecfb4e/llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.41.1 numba-0.58.1 shap-0.43.0 slicer-0.0.7\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730a9cea-0964-48c0-89ef-4f5841f00f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202M/202M [00:00<00:00, 204MiB/s]\n",
      "ğŸ¬  ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨è§£å‹...\n",
      "ğŸ  æ•°æ®é›†å·²ç»æˆåŠŸæ·»åŠ \n"
     ]
    }
   ],
   "source": [
    "!featurize dataset download 0b3523e6-71ba-4e66-9fab-fa73ecc0f58c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55daa61c-405d-4d12-8bb8-58fda84593b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------ç¬¬1è½®è®­ç»ƒå¼€å§‹-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 272/272 [01:57<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒæ¬¡æ•°ï¼š272, Loss: 0.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•æ¬¡æ•°ï¼š68ï¼ŒLossï¼š35.1699\n",
      "1 test acc: 0.8300431654676259\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from resnet import resnet50 as self_resnet50\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////\n",
    "# è‡ªå®šä¹‰æ•°æ®é›†ç±»\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            files = os.listdir(class_path)\n",
    "            self.file_paths.extend([os.path.join(class_path, file) for file in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        data = np.loadtxt(file_path).reshape(32, 512)\n",
    "        data = torch.from_numpy(data).float()\n",
    "        return data, label\n",
    "    \n",
    "    def get_info(self, idx):\n",
    "        \"\"\" è¿”å›æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾ \"\"\"\n",
    "        return self.file_paths[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®éšæœºç§å­\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # æ•°æ®é›†ç›®å½•å’Œæ–‡ä»¶è·¯å¾„\n",
    "    #'D:\\PychramProject\\transzero'\n",
    "    # data_dir = 'data/'\n",
    "    # C:\\Users\\ZHY\\Desktop\\data_txt\n",
    "    # data_dir = 'D:/PychramProject/transzero/data55-512/'\n",
    "    data_dir = 'data/datall/'\n",
    "    class_names = os.listdir(data_dir)\n",
    "\n",
    "    # æ„å»ºæ•°æ®é›†\n",
    "    dataset = MyDataset(data_dir)\n",
    "\n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "    train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    batch_size = 256\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    # //////////////////////////////////////////////////////\n",
    "\n",
    "    resnet50 = self_resnet50(num_classes=9)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ç½‘ç»œæ¨¡å‹cuda\n",
    "    resnet50 = resnet50.cuda()\n",
    "\n",
    "    # loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fn = loss_fn.cuda()\n",
    "\n",
    "\n",
    "    # optimizer\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.SGD(resnet50.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, )\n",
    "\n",
    "\n",
    "    # è®¾ç½®ç½‘ç»œè®­ç»ƒçš„ä¸€äº›å‚æ•°\n",
    "    # è®°å½•è®­ç»ƒçš„æ¬¡æ•°\n",
    "    total_train_step = 0\n",
    "    # è®°å½•æµ‹è¯•çš„æ¬¡æ•°\n",
    "    total_test_step = 0\n",
    "    # è®­ç»ƒçš„è½®æ•°\n",
    "    epoch = 1\n",
    "\n",
    "    best_acc = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    probabilities = []\n",
    "    acc = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(\"-------ç¬¬{}è½®è®­ç»ƒå¼€å§‹-------\".format(i + 1))\n",
    "        resnet50.train()\n",
    "        # è®­ç»ƒæ­¥éª¤å¼€å§‹\n",
    "        for data in tqdm(train_dataloader, ncols=100, desc='Train'):\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                # å›¾åƒcudaï¼›æ ‡ç­¾cuda\n",
    "                # è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½è¦æœ‰\n",
    "                imgs = imgs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "            # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "            # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "            outputs = resnet50(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # ä¼˜åŒ–å™¨ä¼˜åŒ–æ¨¡å‹\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_step = total_train_step + 1\n",
    "            # if total_train_step % 100 == 0:\n",
    "        print(\"è®­ç»ƒæ¬¡æ•°ï¼š{}, Loss: {:.4f}\".format(total_train_step, loss.item()))\n",
    "                # writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "        # æµ‹è¯•é›†\n",
    "        # predictions = []\n",
    "        # targets = []\n",
    "        # probabilities = []\n",
    "        #\n",
    "        resnet50.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            # test\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "\n",
    "            for data in test_dataloader:\n",
    "                imgs, targets = data\n",
    "                if torch.cuda.is_available():\n",
    "                    # å›¾åƒcudaï¼›æ ‡ç­¾cuda\n",
    "                    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½è¦æœ‰\n",
    "                    imgs = imgs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "                # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "                # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "                outputs = resnet50(imgs)\n",
    "                #\n",
    "                # intermediate_layer = resnet50.layer4[-1].conv3  # Modify this line to select the desired intermediate layer\n",
    "                # visualize_features(outputs)\n",
    "                #\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.tolist())\n",
    "                #\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                total_test_loss += loss.item()\n",
    "                total_test_step += 1\n",
    "\n",
    "                # logits = resnet50(imgs)\n",
    "                logits = outputs\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct = torch.eq(pred, targets).float().sum().item()\n",
    "                total_correct += correct\n",
    "                total_num += imgs.size(0)\n",
    "                #\n",
    "                softmax = nn.Softmax(dim=1)\n",
    "                probs = softmax(outputs)\n",
    "                probabilities.extend(probs.tolist())\n",
    "\n",
    "\n",
    "                # if total_test_step % 100 == 0:\n",
    "            print(\"æµ‹è¯•æ¬¡æ•°ï¼š{}ï¼ŒLossï¼š{:.4f}\".format(total_test_step, total_test_loss))\n",
    "            acc = total_correct / total_num\n",
    "            print(epoch, 'test acc:', acc)\n",
    "            #\n",
    "            # print(\"å®šæ€§é¢„æµ‹ç»“æœï¼š\", predictions)\n",
    "            # print(\"å®šé‡é¢„æµ‹ç»“æœï¼š\", probabilities)\n",
    "            # ä¿å­˜æœ€ä¼˜æ¨¡å‹\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(resnet50.state_dict(), 'best_model.pt')\n",
    "\n",
    "    # å°†ç»“æœå†™å…¥æ–‡ä»¶\n",
    "    result_file = 'result.txt'\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "        f.write(\"å®šæ€§é¢„æµ‹ç»“æœï¼š\\n\")\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred}+','\")\n",
    "        f.write(\"å®šé‡é¢„æµ‹ç»“æœï¼š\\n\")\n",
    "        for prob in probabilities:\n",
    "            f.write(f\"{prob}+','\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35013bc-a767-4079-b6c7-514a6c26b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580/580 [15:50:23<00:00, 98.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from resnet import resnet50 as self_resnet50\n",
    "from Resnet50data3 import MyDataset\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# æ•°æ®é›†ç›®å½•å’Œæ–‡ä»¶è·¯å¾„\n",
    "data_dir = 'data/datall/'\n",
    "class_names = os.listdir(data_dir)\n",
    "\n",
    "# æ„å»ºæ•°æ®é›†\n",
    "dataset = MyDataset(data_dir)\n",
    "\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=30, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = self_resnet50(num_classes=9)\n",
    "state = torch.load('best_modell.pt', map_location=device)  # ç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡\n",
    "resnet50.load_state_dict(state)\n",
    "resnet50.to(device).eval()\n",
    "\n",
    "iter_train = iter(train_dataloader)\n",
    "batch_train = next(iter_train)\n",
    "background = None\n",
    "for i in range(100):\n",
    "    image_, _ = batch_train\n",
    "    background = image_ if background is None else torch.cat((background, image_), dim=0)\n",
    "    batch_train = next(iter_train)\n",
    "\n",
    "del iter_train\n",
    "del train_dataloader\n",
    "# e = shap.GradientExplainer(resnet50, background.reshape(-1, 1, 32, 512).cuda())\n",
    "e = shap.GradientExplainer(resnet50, background.reshape(-1, 1, 32, 512).to(device))  # ç¡®ä¿èƒŒæ™¯æ•°æ®åœ¨GPUä¸Š\n",
    "\n",
    "# write test\n",
    "temps = {}\n",
    "count = 0\n",
    "for i, (images, labels) in enumerate(tqdm(test_dataloader, ncols=100, desc=\"cnts\")):\n",
    "\n",
    "    images = images.to(device)  # ç¡®ä¿å›¾åƒåœ¨GPUä¸Š\n",
    "    # è°ƒç”¨å‰å°è¯•å…‹éš†è¾“å…¥çš„å¼ é‡\n",
    "    cloned_images = images.clone().reshape(-1, 1, 32, 512)  # ä½¿ç”¨å…‹éš†å¹¶é‡å¡‘å›¾åƒçš„å‰¯æœ¬\n",
    "    shap_values = e.shap_values(cloned_images)  # ç¡®ä¿ä½¿ç”¨GPUä¸Šçš„æ•°æ®è®¡ç®—SHAPå€¼\n",
    "\n",
    "    for j, l in enumerate(labels):\n",
    "        value = shap_values[l][j][0]\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            value = value.cpu().numpy()  # å°†ç»“æœè½¬ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºNumPyæ•°ç»„\n",
    "        filename, _ = dataset.get_info(test_indices[count])\n",
    "        if int(l) not in temps.keys():\n",
    "            temps[int(l)] = []\n",
    "        temps[int(l)].append([filename, int(l), value])\n",
    "        count += 1\n",
    "\n",
    "\n",
    "import pickle as pkl\n",
    "with open('temp.pkl', 'wb') as f:\n",
    "    pkl.dump(temps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a4d430-5259-4c36-8a75-850e12ee1133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: [13623.79664855  3827.64486606  3511.75438845  2782.20875227\n",
      "  2573.47644634  2197.20298206  2104.24202781  1981.46947019\n",
      "  1917.20958301  1724.45580376  1664.41926583  1661.97843529\n",
      "  1660.38629902  1543.68810613  1491.02414757  1392.5480196\n",
      "  1299.1851391   1281.74834583  1248.10392086  1242.32749654\n",
      "  1240.7238303   1204.14855015  1184.52369425  1099.78604379\n",
      "  1098.29382734  1060.18043611   988.9955072    960.07658422\n",
      "   958.04786774   908.7179597    872.68022879   861.79365579\n",
      "   781.47949123   711.25758849   674.64443257   668.14364201\n",
      "   664.73660849   610.33245464   583.43804327   580.4575966\n",
      "   550.86618266   547.40051955   535.59914335   526.48282892\n",
      "   526.12965875   502.19834428   491.4594207    489.92888326\n",
      "   479.98866585   478.65482295   474.73095391   474.72176852\n",
      "   462.48782735   458.07705267   454.0029156    445.13377533\n",
      "   443.15948578   436.17507953   426.97409601   425.94693833\n",
      "   404.74377253   396.51640981   365.01819747   363.88622236\n",
      "   361.05703794   358.73780244   346.87693629   343.81823669\n",
      "   340.49273294   332.63072379   325.66705986   316.09938342\n",
      "   315.53106735   293.15435547   289.68800152   283.82278356\n",
      "   282.75655693   280.58736195   275.68541454   275.42922084\n",
      "   271.66955469   270.6022164    261.27729853   260.7605806\n",
      "   259.773228     258.87287927   257.02444573   246.47166235\n",
      "   246.08602947   240.09439487   237.9491509    231.4974758\n",
      "   229.68700545   228.29140796   227.10202912   224.90206061\n",
      "   217.98161019   211.92872473   210.56618617   209.95210078\n",
      "   208.51781057   207.70661211   207.64509257   205.86139498\n",
      "   205.52766751   203.56156636   203.03769967   199.06684967\n",
      "   196.87800153   192.81150738   188.9559115    188.76296974\n",
      "   187.94682601   187.13740033   183.51360821   183.06683135\n",
      "   182.20063654   181.69761337   179.00616736   178.04972526\n",
      "   178.03871141   177.65974151   176.89619178   176.3043207\n",
      "   176.18008088   176.06470933   176.03807783   175.49453481\n",
      "   174.8813901    174.4800451    173.99972972   172.63557684\n",
      "   172.20294776   171.3732437    166.74175224   166.35139059\n",
      "   165.53990836   163.87044049   163.49090273   163.16839886\n",
      "   163.07766788   161.34102976   161.24743122   161.21868733\n",
      "   157.94612353   157.74333776   157.11054736   156.87488087\n",
      "   156.47002014   150.04289258   148.82000829   147.5118633\n",
      "   146.80694714   145.98687648   143.39336857   143.17595355\n",
      "   142.80712951   142.26240271   142.12363818   141.87560709\n",
      "   141.53237214   139.89435467   139.25235852   139.21910683\n",
      "   139.04524517   138.40540785   137.76617234   136.77831282\n",
      "   135.67774428   134.17613488   132.40225291   132.08740754\n",
      "   131.94665965   130.92875961   130.37615171   130.19240911\n",
      "   130.1607844    130.14325745   130.13117702   129.5921861 ]\n",
      "index: [ 49  35  33  38   2   9   8  31  37  59  36  28  29  27  55  47 125  41\n",
      "  45  11  44  32   4  40  39  10  25  46  58  63  34   5  51  20 427  19\n",
      "  69 129  14 489  26  56  67  43   7 145  23  93 483  18  50   6  94  68\n",
      " 387  96  30  92 395  95 124  66  65  57  64 117 328  62  12  70  52 413\n",
      "  61  73 391  77 473 320 197 288 149 424  90  72 422 450 390 477  71 221\n",
      " 435 486  24 159 211  78  85  13 241  21  74 240  97 283  86 287 163  91\n",
      "   1  82 259 267  76 384 103  42 133 301 296  87 484 260 109  60 210 445\n",
      " 193 107 443 423 156 132 161 298 434 490 421 126  15 479 441  80  75 316\n",
      " 253 147 453 471 363 350 153 104 105 293 158 331 122 297 138 263 305 166\n",
      " 134  17 209 493 403 127 225  88 345 358 485  98 115 135  99 481 467 262]\n",
      "****************************************************************************************************\n",
      "æŠ¥å¤´å­—æ®µä¸º: [49 35 33 38  2  9  8 31 37 36 28 29 27 47 41 45 11 44 32  4 40 39 10 25\n",
      " 46 34  5 51 20 19 14 26 43  7 23 18 50  6 30 12 52 24 13 21  1 42 15 17]\n",
      "****************************************************************************************************\n",
      "æ’åºåå­—æ®µä¸º: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49 50 51 52]\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "TopK = 180\n",
    "num_classes = 9\n",
    "\n",
    "with open('temp.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "# -------------- ä¹‹å‰çš„æ–¹æ³• ----------------- # \n",
    "# results = {}  # ä¿å­˜æ¯ç±»å‰20\n",
    "# for i in range(9):\n",
    "#     results[i] = {}\n",
    "#     samples = data[i]  # è·å–æ¯ç±»ä¿å­˜çš„æ•°æ® [filename, label, data(32, 512)]\n",
    "#     d_array = [samples[k][-1] for k in range(len(samples))]\n",
    "\n",
    "#     # å…ˆæ±‚å’Œï¼Œå†æ’åº\n",
    "#     d_sum = np.sum(d_array, axis=0)  # shape =ï¼ˆ512, ï¼‰\n",
    "#     d_sort_value = np.sort(d_sum)[::-1]    # è´¡çŒ®å€¼, ä»å¤§åˆ°å°\n",
    "#     d_sort_index = np.argsort(d_sum)[::-1] # å­—æ®µç´¢å¼•, ä»å¤§åˆ°å°\n",
    "\n",
    "\n",
    "#     results[i]['contribute'] = d_sort_value\n",
    "#     results[i]['index'] = d_sort_index\n",
    "\n",
    "\n",
    "# print(results)\n",
    "# -------------- ä¹‹å‰çš„æ–¹æ³• ----------------- # \n",
    "\n",
    "\n",
    "# -------------- ä¿®æ”¹åçš„æ–¹æ³• ----------------- # \n",
    "results = []\n",
    "for i in range(num_classes):\n",
    "    samples = data[i]  # è·å–æ¯ç±»ä¿å­˜çš„æ•°æ® [filename, label, data(32, 512)]\n",
    "    d_list = [samples[k][-1] for k in range(len(samples))]\n",
    "    results += d_list\n",
    "\n",
    "\n",
    "# step 1: (n, 32, 512)\n",
    "results_array = np.array(results)\n",
    "# step 2: (1, 32, 512), æŒ‰ç¬¬ä¸€ç»´åº¦æ±‚å’Œå¾—åˆ° (32, 512)\n",
    "results_array = np.sum(results_array, axis=0)\n",
    "# step 3: (32, 512), æŒ‰ç¬¬ä¸€ç»´åº¦å–æœ€å¤§å€¼å¾—åˆ° (512, )\n",
    "results_array = np.max(results_array, axis=0)\n",
    "# step 4: (512, )æŒ‰å¤§å°æ’åºï¼Œå–å‰20\n",
    "sort_value = np.sort(results_array)[::-1]    # è´¡çŒ®å€¼è¿›è¡Œä»å¤§åˆ°å°çš„æ’åº\n",
    "sort_index = np.argsort(results_array)[::-1] # è´¡çŒ®å€¼çš„ç´¢å¼•ä»å¤§åˆ°å°çš„æ’åº\n",
    "\n",
    "sort_index += 1\n",
    "\n",
    "print(\"value: {}\".format(sort_value[:TopK]))\n",
    "print(\"index: {}\".format(sort_index[:TopK]))\n",
    "\n",
    "# è¾“å‡ºindexä¸­å°äº55çš„å€¼\n",
    "print(\"*\"*100)\n",
    "index_topK = sort_index[:TopK]\n",
    "less_than_55 = index_topK[index_topK < 55]\n",
    "less_than_55_sort = np.sort(less_than_55)\n",
    "print(\"æŠ¥å¤´å­—æ®µä¸º: {}\".format(less_than_55))\n",
    "print(\"*\"*100)\n",
    "print(\"æ’åºåå­—æ®µä¸º: {}\".format(less_than_55_sort))\n",
    "\n",
    "# -------------- ä¿®æ”¹åçš„æ–¹æ³• ----------------- # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504da7e-494f-40f2-ac10-227d6a318ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
