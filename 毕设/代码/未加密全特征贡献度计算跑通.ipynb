{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d0996a-3636-4211-ac77-021444a8bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting shap\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9e/3f/247e0017d52eeef37c170d71357eb3a12a2c06718d2e184c9929b6f3d9ed/shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.24.1)\n",
      "Requirement already satisfied: scipy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (from shap) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (23.0)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/13/b66627125b35f2987bd9872cf028b5e1e1ffcbc8d1e182ac4e84eed3998f/numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba->shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/57/7d/ef28d5812f852b93bd2a583d00cdcde56833d31b645ae0eaa7e71eecfb4e/llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.41.1 numba-0.58.1 shap-0.43.0 slicer-0.0.7\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730a9cea-0964-48c0-89ef-4f5841f00f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 202M/202M [00:00<00:00, 204MiB/s]\n",
      "🍬  下载完成，正在解压...\n",
      "🏁  数据集已经成功添加\n"
     ]
    }
   ],
   "source": [
    "!featurize dataset download 0b3523e6-71ba-4e66-9fab-fa73ecc0f58c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55daa61c-405d-4d12-8bb8-58fda84593b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第1轮训练开始-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████| 272/272 [01:57<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数：272, Loss: 0.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试次数：68，Loss：35.1699\n",
      "1 test acc: 0.8300431654676259\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from resnet import resnet50 as self_resnet50\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////\n",
    "# 自定义数据集类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            files = os.listdir(class_path)\n",
    "            self.file_paths.extend([os.path.join(class_path, file) for file in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        data = np.loadtxt(file_path).reshape(32, 512)\n",
    "        data = torch.from_numpy(data).float()\n",
    "        return data, label\n",
    "    \n",
    "    def get_info(self, idx):\n",
    "        \"\"\" 返回文件路径和标签 \"\"\"\n",
    "        return self.file_paths[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 数据集目录和文件路径\n",
    "    #'D:\\PychramProject\\transzero'\n",
    "    # data_dir = 'data/'\n",
    "    # C:\\Users\\ZHY\\Desktop\\data_txt\n",
    "    # data_dir = 'D:/PychramProject/transzero/data55-512/'\n",
    "    data_dir = 'data/datall/'\n",
    "    class_names = os.listdir(data_dir)\n",
    "\n",
    "    # 构建数据集\n",
    "    dataset = MyDataset(data_dir)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    # 创建数据加载器\n",
    "    batch_size = 256\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    # //////////////////////////////////////////////////////\n",
    "\n",
    "    resnet50 = self_resnet50(num_classes=9)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 网络模型cuda\n",
    "    resnet50 = resnet50.cuda()\n",
    "\n",
    "    # loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fn = loss_fn.cuda()\n",
    "\n",
    "\n",
    "    # optimizer\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.SGD(resnet50.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, )\n",
    "\n",
    "\n",
    "    # 设置网络训练的一些参数\n",
    "    # 记录训练的次数\n",
    "    total_train_step = 0\n",
    "    # 记录测试的次数\n",
    "    total_test_step = 0\n",
    "    # 训练的轮数\n",
    "    epoch = 1\n",
    "\n",
    "    best_acc = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    probabilities = []\n",
    "    acc = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(\"-------第{}轮训练开始-------\".format(i + 1))\n",
    "        resnet50.train()\n",
    "        # 训练步骤开始\n",
    "        for data in tqdm(train_dataloader, ncols=100, desc='Train'):\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                # 图像cuda；标签cuda\n",
    "                # 训练集和测试集都要有\n",
    "                imgs = imgs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "            # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "            # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "            outputs = resnet50(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # 优化器优化模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_step = total_train_step + 1\n",
    "            # if total_train_step % 100 == 0:\n",
    "        print(\"训练次数：{}, Loss: {:.4f}\".format(total_train_step, loss.item()))\n",
    "                # writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "        # 测试集\n",
    "        # predictions = []\n",
    "        # targets = []\n",
    "        # probabilities = []\n",
    "        #\n",
    "        resnet50.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            # test\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "\n",
    "            for data in test_dataloader:\n",
    "                imgs, targets = data\n",
    "                if torch.cuda.is_available():\n",
    "                    # 图像cuda；标签cuda\n",
    "                    # 训练集和测试集都要有\n",
    "                    imgs = imgs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "                # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "                # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "                outputs = resnet50(imgs)\n",
    "                #\n",
    "                # intermediate_layer = resnet50.layer4[-1].conv3  # Modify this line to select the desired intermediate layer\n",
    "                # visualize_features(outputs)\n",
    "                #\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.tolist())\n",
    "                #\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                total_test_loss += loss.item()\n",
    "                total_test_step += 1\n",
    "\n",
    "                # logits = resnet50(imgs)\n",
    "                logits = outputs\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct = torch.eq(pred, targets).float().sum().item()\n",
    "                total_correct += correct\n",
    "                total_num += imgs.size(0)\n",
    "                #\n",
    "                softmax = nn.Softmax(dim=1)\n",
    "                probs = softmax(outputs)\n",
    "                probabilities.extend(probs.tolist())\n",
    "\n",
    "\n",
    "                # if total_test_step % 100 == 0:\n",
    "            print(\"测试次数：{}，Loss：{:.4f}\".format(total_test_step, total_test_loss))\n",
    "            acc = total_correct / total_num\n",
    "            print(epoch, 'test acc:', acc)\n",
    "            #\n",
    "            # print(\"定性预测结果：\", predictions)\n",
    "            # print(\"定量预测结果：\", probabilities)\n",
    "            # 保存最优模型\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(resnet50.state_dict(), 'best_model.pt')\n",
    "\n",
    "    # 将结果写入文件\n",
    "    result_file = 'result.txt'\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "        f.write(\"定性预测结果：\\n\")\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{pred}+','\")\n",
    "        f.write(\"定量预测结果：\\n\")\n",
    "        for prob in probabilities:\n",
    "            f.write(f\"{prob}+','\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35013bc-a767-4079-b6c7-514a6c26b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cnts: 100%|████████████████████████████████████████████████████| 580/580 [15:50:23<00:00, 98.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from resnet import resnet50 as self_resnet50\n",
    "from Resnet50data3 import MyDataset\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 数据集目录和文件路径\n",
    "data_dir = 'data/datall/'\n",
    "class_names = os.listdir(data_dir)\n",
    "\n",
    "# 构建数据集\n",
    "dataset = MyDataset(data_dir)\n",
    "\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=30, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = self_resnet50(num_classes=9)\n",
    "state = torch.load('best_modell.pt', map_location=device)  # 直接加载到指定设备\n",
    "resnet50.load_state_dict(state)\n",
    "resnet50.to(device).eval()\n",
    "\n",
    "iter_train = iter(train_dataloader)\n",
    "batch_train = next(iter_train)\n",
    "background = None\n",
    "for i in range(100):\n",
    "    image_, _ = batch_train\n",
    "    background = image_ if background is None else torch.cat((background, image_), dim=0)\n",
    "    batch_train = next(iter_train)\n",
    "\n",
    "del iter_train\n",
    "del train_dataloader\n",
    "# e = shap.GradientExplainer(resnet50, background.reshape(-1, 1, 32, 512).cuda())\n",
    "e = shap.GradientExplainer(resnet50, background.reshape(-1, 1, 32, 512).to(device))  # 确保背景数据在GPU上\n",
    "\n",
    "# write test\n",
    "temps = {}\n",
    "count = 0\n",
    "for i, (images, labels) in enumerate(tqdm(test_dataloader, ncols=100, desc=\"cnts\")):\n",
    "\n",
    "    images = images.to(device)  # 确保图像在GPU上\n",
    "    # 调用前尝试克隆输入的张量\n",
    "    cloned_images = images.clone().reshape(-1, 1, 32, 512)  # 使用克隆并重塑图像的副本\n",
    "    shap_values = e.shap_values(cloned_images)  # 确保使用GPU上的数据计算SHAP值\n",
    "\n",
    "    for j, l in enumerate(labels):\n",
    "        value = shap_values[l][j][0]\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            value = value.cpu().numpy()  # 将结果转移到CPU并转换为NumPy数组\n",
    "        filename, _ = dataset.get_info(test_indices[count])\n",
    "        if int(l) not in temps.keys():\n",
    "            temps[int(l)] = []\n",
    "        temps[int(l)].append([filename, int(l), value])\n",
    "        count += 1\n",
    "\n",
    "\n",
    "import pickle as pkl\n",
    "with open('temp.pkl', 'wb') as f:\n",
    "    pkl.dump(temps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a4d430-5259-4c36-8a75-850e12ee1133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: [13623.79664855  3827.64486606  3511.75438845  2782.20875227\n",
      "  2573.47644634  2197.20298206  2104.24202781  1981.46947019\n",
      "  1917.20958301  1724.45580376  1664.41926583  1661.97843529\n",
      "  1660.38629902  1543.68810613  1491.02414757  1392.5480196\n",
      "  1299.1851391   1281.74834583  1248.10392086  1242.32749654\n",
      "  1240.7238303   1204.14855015  1184.52369425  1099.78604379\n",
      "  1098.29382734  1060.18043611   988.9955072    960.07658422\n",
      "   958.04786774   908.7179597    872.68022879   861.79365579\n",
      "   781.47949123   711.25758849   674.64443257   668.14364201\n",
      "   664.73660849   610.33245464   583.43804327   580.4575966\n",
      "   550.86618266   547.40051955   535.59914335   526.48282892\n",
      "   526.12965875   502.19834428   491.4594207    489.92888326\n",
      "   479.98866585   478.65482295   474.73095391   474.72176852\n",
      "   462.48782735   458.07705267   454.0029156    445.13377533\n",
      "   443.15948578   436.17507953   426.97409601   425.94693833\n",
      "   404.74377253   396.51640981   365.01819747   363.88622236\n",
      "   361.05703794   358.73780244   346.87693629   343.81823669\n",
      "   340.49273294   332.63072379   325.66705986   316.09938342\n",
      "   315.53106735   293.15435547   289.68800152   283.82278356\n",
      "   282.75655693   280.58736195   275.68541454   275.42922084\n",
      "   271.66955469   270.6022164    261.27729853   260.7605806\n",
      "   259.773228     258.87287927   257.02444573   246.47166235\n",
      "   246.08602947   240.09439487   237.9491509    231.4974758\n",
      "   229.68700545   228.29140796   227.10202912   224.90206061\n",
      "   217.98161019   211.92872473   210.56618617   209.95210078\n",
      "   208.51781057   207.70661211   207.64509257   205.86139498\n",
      "   205.52766751   203.56156636   203.03769967   199.06684967\n",
      "   196.87800153   192.81150738   188.9559115    188.76296974\n",
      "   187.94682601   187.13740033   183.51360821   183.06683135\n",
      "   182.20063654   181.69761337   179.00616736   178.04972526\n",
      "   178.03871141   177.65974151   176.89619178   176.3043207\n",
      "   176.18008088   176.06470933   176.03807783   175.49453481\n",
      "   174.8813901    174.4800451    173.99972972   172.63557684\n",
      "   172.20294776   171.3732437    166.74175224   166.35139059\n",
      "   165.53990836   163.87044049   163.49090273   163.16839886\n",
      "   163.07766788   161.34102976   161.24743122   161.21868733\n",
      "   157.94612353   157.74333776   157.11054736   156.87488087\n",
      "   156.47002014   150.04289258   148.82000829   147.5118633\n",
      "   146.80694714   145.98687648   143.39336857   143.17595355\n",
      "   142.80712951   142.26240271   142.12363818   141.87560709\n",
      "   141.53237214   139.89435467   139.25235852   139.21910683\n",
      "   139.04524517   138.40540785   137.76617234   136.77831282\n",
      "   135.67774428   134.17613488   132.40225291   132.08740754\n",
      "   131.94665965   130.92875961   130.37615171   130.19240911\n",
      "   130.1607844    130.14325745   130.13117702   129.5921861 ]\n",
      "index: [ 49  35  33  38   2   9   8  31  37  59  36  28  29  27  55  47 125  41\n",
      "  45  11  44  32   4  40  39  10  25  46  58  63  34   5  51  20 427  19\n",
      "  69 129  14 489  26  56  67  43   7 145  23  93 483  18  50   6  94  68\n",
      " 387  96  30  92 395  95 124  66  65  57  64 117 328  62  12  70  52 413\n",
      "  61  73 391  77 473 320 197 288 149 424  90  72 422 450 390 477  71 221\n",
      " 435 486  24 159 211  78  85  13 241  21  74 240  97 283  86 287 163  91\n",
      "   1  82 259 267  76 384 103  42 133 301 296  87 484 260 109  60 210 445\n",
      " 193 107 443 423 156 132 161 298 434 490 421 126  15 479 441  80  75 316\n",
      " 253 147 453 471 363 350 153 104 105 293 158 331 122 297 138 263 305 166\n",
      " 134  17 209 493 403 127 225  88 345 358 485  98 115 135  99 481 467 262]\n",
      "****************************************************************************************************\n",
      "报头字段为: [49 35 33 38  2  9  8 31 37 36 28 29 27 47 41 45 11 44 32  4 40 39 10 25\n",
      " 46 34  5 51 20 19 14 26 43  7 23 18 50  6 30 12 52 24 13 21  1 42 15 17]\n",
      "****************************************************************************************************\n",
      "排序后字段为: [ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27\n",
      " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49 50 51 52]\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "TopK = 180\n",
    "num_classes = 9\n",
    "\n",
    "with open('temp.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "# -------------- 之前的方法 ----------------- # \n",
    "# results = {}  # 保存每类前20\n",
    "# for i in range(9):\n",
    "#     results[i] = {}\n",
    "#     samples = data[i]  # 获取每类保存的数据 [filename, label, data(32, 512)]\n",
    "#     d_array = [samples[k][-1] for k in range(len(samples))]\n",
    "\n",
    "#     # 先求和，再排序\n",
    "#     d_sum = np.sum(d_array, axis=0)  # shape =（512, ）\n",
    "#     d_sort_value = np.sort(d_sum)[::-1]    # 贡献值, 从大到小\n",
    "#     d_sort_index = np.argsort(d_sum)[::-1] # 字段索引, 从大到小\n",
    "\n",
    "\n",
    "#     results[i]['contribute'] = d_sort_value\n",
    "#     results[i]['index'] = d_sort_index\n",
    "\n",
    "\n",
    "# print(results)\n",
    "# -------------- 之前的方法 ----------------- # \n",
    "\n",
    "\n",
    "# -------------- 修改后的方法 ----------------- # \n",
    "results = []\n",
    "for i in range(num_classes):\n",
    "    samples = data[i]  # 获取每类保存的数据 [filename, label, data(32, 512)]\n",
    "    d_list = [samples[k][-1] for k in range(len(samples))]\n",
    "    results += d_list\n",
    "\n",
    "\n",
    "# step 1: (n, 32, 512)\n",
    "results_array = np.array(results)\n",
    "# step 2: (1, 32, 512), 按第一维度求和得到 (32, 512)\n",
    "results_array = np.sum(results_array, axis=0)\n",
    "# step 3: (32, 512), 按第一维度取最大值得到 (512, )\n",
    "results_array = np.max(results_array, axis=0)\n",
    "# step 4: (512, )按大小排序，取前20\n",
    "sort_value = np.sort(results_array)[::-1]    # 贡献值进行从大到小的排序\n",
    "sort_index = np.argsort(results_array)[::-1] # 贡献值的索引从大到小的排序\n",
    "\n",
    "sort_index += 1\n",
    "\n",
    "print(\"value: {}\".format(sort_value[:TopK]))\n",
    "print(\"index: {}\".format(sort_index[:TopK]))\n",
    "\n",
    "# 输出index中小于55的值\n",
    "print(\"*\"*100)\n",
    "index_topK = sort_index[:TopK]\n",
    "less_than_55 = index_topK[index_topK < 55]\n",
    "less_than_55_sort = np.sort(less_than_55)\n",
    "print(\"报头字段为: {}\".format(less_than_55))\n",
    "print(\"*\"*100)\n",
    "print(\"排序后字段为: {}\".format(less_than_55_sort))\n",
    "\n",
    "# -------------- 修改后的方法 ----------------- # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504da7e-494f-40f2-ac10-227d6a318ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
