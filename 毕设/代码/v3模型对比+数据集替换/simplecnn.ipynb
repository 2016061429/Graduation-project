{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0ee7ce-58a3-4293-992f-ca9fb97751e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202M/202M [00:00<00:00, 389MiB/s]\n",
      "üç¨  ‰∏ãËΩΩÂÆåÊàêÔºåÊ≠£Âú®Ëß£Âéã...\n",
      "üèÅ  Êï∞ÊçÆÈõÜÂ∑≤ÁªèÊàêÂäüÊ∑ªÂä†\n"
     ]
    }
   ],
   "source": [
    "!featurize dataset download 0b3523e6-71ba-4e66-9fab-fa73ecc0f58c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c78770-a23d-409b-8c7d-428f6afaf3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting shap\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9e/3f/247e0017d52eeef37c170d71357eb3a12a2c06718d2e184c9929b6f3d9ed/shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.24.1)\n",
      "Requirement already satisfied: scipy in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (from shap) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in /environment/miniconda3/lib/python3.10/site-packages (from shap) (23.0)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/13/b66627125b35f2987bd9872cf028b5e1e1ffcbc8d1e182ac4e84eed3998f/numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle (from shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/43/dae06432d0c4b1dc9e9149ad37b4ca8384cf6eb7700cd9215b177b914f0a/cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba->shap)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/57/7d/ef28d5812f852b93bd2a583d00cdcde56833d31b645ae0eaa7e71eecfb4e/llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.41.1 numba-0.58.1 shap-0.43.0 slicer-0.0.7\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-learn in /environment/miniconda3/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9775913f-597b-4fec-838c-ed8c39380d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Á¨¨1ËΩÆËÆ≠ÁªÉÂºÄÂßã-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [00:33<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËÆ≠ÁªÉÊ¨°Êï∞Ôºö272, Loss: 1.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊµãËØïÊ¨°Êï∞Ôºö68ÔºåLossÔºö123.2071\n",
      "1 test acc: 0.3620719424460432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from resnet import resnet50 as self_resnet50\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from simple_cnn import SimpleCNN  # ÂºïÂÖ•SimpleCNNÁ±ª\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////\n",
    "# Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜÁ±ª\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            files = os.listdir(class_path)\n",
    "            self.file_paths.extend([os.path.join(class_path, file) for file in files])\n",
    "            self.labels.extend([i] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        data = np.loadtxt(file_path).reshape(32, 512)\n",
    "        data = torch.from_numpy(data).float()\n",
    "        return data, label\n",
    "    \n",
    "    def get_info(self, idx):\n",
    "        \"\"\" ËøîÂõûÊñá‰ª∂Ë∑ØÂæÑÂíåÊ†áÁ≠æ \"\"\"\n",
    "        return self.file_paths[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Êï∞ÊçÆÈõÜÁõÆÂΩïÂíåÊñá‰ª∂Ë∑ØÂæÑ\n",
    "    #'D:\\PychramProject\\transzero'\n",
    "    # data_dir = 'data/'\n",
    "    # C:\\Users\\ZHY\\Desktop\\data_txt\n",
    "    # data_dir = 'D:/PychramProject/transzero/data55-512/'\n",
    "    data_dir = 'data/datall/'\n",
    "    class_names = os.listdir(data_dir)\n",
    "\n",
    "    # ÊûÑÂª∫Êï∞ÊçÆÈõÜ\n",
    "    dataset = MyDataset(data_dir)\n",
    "\n",
    "    # ÂàíÂàÜËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ\n",
    "    train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®\n",
    "    batch_size = 256\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    # //////////////////////////////////////////////////////\n",
    "\n",
    "    model = SimpleCNN(num_classes=9)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ÁΩëÁªúÊ®°Âûãcuda\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fn = loss_fn.cuda()\n",
    "\n",
    "\n",
    "    # optimizer\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, )\n",
    "\n",
    "\n",
    "    # ËÆæÁΩÆÁΩëÁªúËÆ≠ÁªÉÁöÑ‰∏Ä‰∫õÂèÇÊï∞\n",
    "    # ËÆ∞ÂΩïËÆ≠ÁªÉÁöÑÊ¨°Êï∞\n",
    "    total_train_step = 0\n",
    "    # ËÆ∞ÂΩïÊµãËØïÁöÑÊ¨°Êï∞\n",
    "    total_test_step = 0\n",
    "    # ËÆ≠ÁªÉÁöÑËΩÆÊï∞\n",
    "    epoch = 1\n",
    "\n",
    "    best_acc = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    probabilities = []\n",
    "    acc = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print(\"-------Á¨¨{}ËΩÆËÆ≠ÁªÉÂºÄÂßã-------\".format(i + 1))\n",
    "        model.train()\n",
    "        # ËÆ≠ÁªÉÊ≠•È™§ÂºÄÂßã\n",
    "        for data in tqdm(train_dataloader, ncols=100, desc='Train'):\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():\n",
    "                # ÂõæÂÉècudaÔºõÊ†áÁ≠æcuda\n",
    "                # ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÈÉΩË¶ÅÊúâ\n",
    "                imgs = imgs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "            # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "            # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # ‰ºòÂåñÂô®‰ºòÂåñÊ®°Âûã\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_step = total_train_step + 1\n",
    "            # if total_train_step % 100 == 0:\n",
    "        print(\"ËÆ≠ÁªÉÊ¨°Êï∞Ôºö{}, Loss: {:.4f}\".format(total_train_step, loss.item()))\n",
    "                # writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "        # ÊµãËØïÈõÜ\n",
    "        # predictions = []\n",
    "        # targets = []\n",
    "        # probabilities = []\n",
    "        #\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            # test\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "\n",
    "            for data in test_dataloader:\n",
    "                imgs, targets = data\n",
    "                if torch.cuda.is_available():\n",
    "                    # ÂõæÂÉècudaÔºõÊ†áÁ≠æcuda\n",
    "                    # ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÈÉΩË¶ÅÊúâ\n",
    "                    imgs = imgs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                imgs = imgs.reshape(-1, 1, 32, 512)\n",
    "                # imgs1 = torch.cat((imgs, imgs), 1)\n",
    "                # imgs2 = torch.cat((imgs, imgs1), 1)\n",
    "                outputs = model(imgs)\n",
    "                #\n",
    "                # intermediate_layer = model.layer4[-1].conv3  # Modify this line to select the desired intermediate layer\n",
    "                # visualize_features(outputs)\n",
    "                #\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.tolist())\n",
    "                #\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                total_test_loss += loss.item()\n",
    "                total_test_step += 1\n",
    "\n",
    "                # logits = model(imgs)\n",
    "                logits = outputs\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct = torch.eq(pred, targets).float().sum().item()\n",
    "                total_correct += correct\n",
    "                total_num += imgs.size(0)\n",
    "                #\n",
    "                softmax = nn.Softmax(dim=1)\n",
    "                probs = softmax(outputs)\n",
    "                probabilities.extend(probs.tolist())\n",
    "\n",
    "\n",
    "                # if total_test_step % 100 == 0:\n",
    "            print(\"ÊµãËØïÊ¨°Êï∞Ôºö{}ÔºåLossÔºö{:.4f}\".format(total_test_step, total_test_loss))\n",
    "            acc = total_correct / total_num\n",
    "            print(epoch, 'test acc:', acc)\n",
    "            #\n",
    "            # print(\"ÂÆöÊÄßÈ¢ÑÊµãÁªìÊûúÔºö\", predictions)\n",
    "            # print(\"ÂÆöÈáèÈ¢ÑÊµãÁªìÊûúÔºö\", probabilities)\n",
    "            # ‰øùÂ≠òÊúÄ‰ºòÊ®°Âûã\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656394f-b04b-4281-8616-83f33fd6ba23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
